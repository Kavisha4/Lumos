# Lumos – Blind Navigation Assistance App

---

## 🛑 Problem Statement
Visually impaired individuals face challenges navigating urban areas due to obstacles like stairs, vehicles, and uneven pathways. Existing aids provide limited awareness. **Lumos** offers real-time, accessible guidance via audio.

---

## 💡 Solution
**Lumos** is a Flutter-based AI mobile app that:

- Captures surroundings using the smartphone camera.
- Detects objects like doors, stairs, vehicles, and obstacles in real-time.
- Converts visual data into audio cues for safer navigation.
- Supports multiple languages and haptic feedback for contextual guidance.

---

## ⚙️ Tech Stack
- **Framework:** Flutter  
- **AI/ML:** TensorFlow Lite, MobileNet-SSD, Google Gemini Pro Vision API  
- **Device APIs:** Camera, TTS (Text-to-Speech), Vibration, Sound  
- **Persistence & Config:** AsyncStorage, React Native Config (if needed)  
- **Version Control & IDE:** Git, GitHub, VS Code  

---

## 🚀 Key Features
- Live object detection and environment analysis  
- Audio and haptic feedback in real-time  
- Supports urban and rural navigation  
- Lightweight and efficient on-device processing  

---

## 🌐 Business Scope
- **NGOs & Accessibility Foundations:** Empower visually impaired communities  
- **CSR Initiatives:** Companies can sponsor or integrate Lumos  
- **Public Sector & Smart Cities:** Inclusion in city accessibility projects  
- **Monetization:** Freemium app with optional premium features; B2B licensing to public venues; data insights for urban planning  

---

## 📱 Contact
Reach out via the email or Linkedin.
